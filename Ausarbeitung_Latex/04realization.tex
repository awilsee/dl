\mysection{Realization}
In general, this chapter describes the methodical procedure of solving the above mentioned problem in \subsecref{Terms of Referencee}. After describing the used dataset all required software and hardware components are explained in detail. Furthermore, the chapter leads through the installation steps of Tensorflow and the setup of Android Studio. Followed by the installation process the retraining of a pre-trained model is depicted. Afterwards, the re-trained model is tested and validated. The chapter ends with the description of the realization of the Android app.

	\mysubsection{dataset}
The most famous image dataset is ImageNet which is known from the ILSVRC2012. It contains overall about 14 million Images \citep{ImageNet2010}. This dataset is also used for nearly all pre-trained models. The dataset includes also a set of 120 different dog breeds with about 150 images each breed. This dog dataset can be downloaded from the following Stanford website \url{http://vision.stanford.edu/aditya86/ImageNetDogs/}. \\
Another dataset with dogs is from udacity and can be downloaded from \url{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip}. It contains 133 dog breeds with a mean of 63 images each breed. The images in this dataset are completely different from the ImageNet dataset. \\

Because of the reason that the ImageNet dataset is already used to train the pretrained models, the last dataset is used for this project, which is also fully unknown for any model.

	\mysubsection{hardware environment}
Different hardware setups are used for this project to compare their performance effects.
At first the lowest one is a notebook with a XXXXXXXXCPU @ XXXXX Mhz.
The other one is a powerful desktop computer with an Intel(R) Core(TM) i7-6700K CPU @ 4.4 GHz and 16 GB memory.
In this computer there is also a NVIDIA GeForce GTX 980 with CUDA capabilities version 5.2. It has 2048 CUDA cores running @ 1.291 GHz and 4 GB memory.

For the mobile application also two different mobile phones are used. The first one is a Samsung Galaxy XXXXXX with a XXXXXX and XXXX GB memory. Android XXXXX (API XXX) is running.
The other one is a Motorola Moto X (2nd Gen, 2014) with a Qualcomm Snapdragon 801 quad-core @2.5 GHz (32-bit, ARMv7-ISA) and 2 GB memory. The operating system is Android 6.0 (API 23).


	\mysubsection{installation of software} Andi
This chapter includes all necessary steps for installing the software environment including Tensorflow.

 			\mysubsubsection{Prerequisites}
The software environment was set up on the Linux distribution Ubuntu 16.04 LTS. To install the software environment for Tensorflow Python is required. Therefore, the current version of Python 3.6 was installed by default. Tensorflow also supports Bazel which was installed by following command.

\begin{lstlisting}[caption=Bazel Installation, label=list:bazel, language=bash]
	sudo apt-get install openjdk-8-jdk
	
	echo "deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8" | sudo tee /etc/apt/				sources.list.d/bazel.list
	curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -
	
	sudo apt-get update && sudo apt-get install bazel
	
	sudo apt-get upgrade bazel
\end{lstlisting}	

Futhermore, the package and environment management tool Anaconda was installed by the following steps:
First, the Anaconda installer was downloaded from \url{https://www.anaconda.com/download/#linux}. During the installation process the prompts were answered by the default suggestions except the following prompt: "Do you wish the installer to prepend the Anaconda3 install location to PATH in your /home/aw/.bashrc ? [yes|no]". "yes" was typed in and conda was tested using the "conda list" command. \\

As an app development environment the free IDE Android Studio in its version 3.0.1 was installed by downloading from \url{https://developer.android.com/studio/index.html}, extracting and following the instruction steps. Required dependencies are installed by Android Studio itself. So, the SDK in the version 26.1.1 was used. \\

In the development of native Android Apps Java is used as the programming language. Within the installation of Anaconda, the JDK in the version 8 was installed.
- test anaconda, tensorflow environment look where it belongs

 	- CUDA, CUDNN
 	
			\mysubsubsection{Tensorflow based on Python}

			\mysubsubsection{Tensorflow based on Bazel}
				- e.g. Workspace changes for Android SDK, msse4.2
			\mysubsubsection{Installing Android Studio and its Delevopment Kit}
				- also possible with bazel but easier Android studio (needs correct versions of sdk, ndk) \\
				- SDK, NDK \\
				- IMPORTANT: tf versions updaten (same as trained)

	\mysubsection{building the models} Alice bis Steps, Andi ab Optimierung, time GPU/CPU
	-> evtl extra subsubsection: \\
		- execution methods -> Bazel and Python (incompatible versions) \\
		- Mobilnet -> steps, optimierung \\
		- Inception -> steps, optimierung \\
		- time related differences of execution  \\
		  -> time CPUs/GPU

	\mysubsection{Output Tests and Validation}
To test and validate the output of the model regardless of whether the pretrained, retrained or optimized one the python script \textit{label\_image} of the 'tensorflow-for-poets'-tutorial is used with following command. Three variables has to be defined differently which is shown at the beginning of \listref{label_image} depending on the used model and if it already has been retrained or not.
\begin{lstlisting}[caption=Call of \textit{label\_image.py}, label=list:label_image, language=bash]
	#for InceptionV3 models	use:
	INPUT_SIZE=299			#size of input layer
	INPUT_LAYER=Mul		#name of input layer
	OUTPUT_LAYER=softmax	#name of output layer (only for pretrained model otherwise 'final_result' or as defined in retrain-script)
	
	#for MobileNet models use:
	INPUT_SIZE=224			#size of input layer
	INPUT_LAYER=input	 	#name of input layer
	OUTPUT_LAYER=MobilenetV1/Predictions/Reshape_1	#name of output layer (only for pretrained model otherwise 'final_result' or as defined in retrain-script)

	python -m scripts.label_image \
	--graph=${HOME}/retrained_graph.pb \
	--labels=${HOME}/retrained_labels.txt \
	--output_layer=final_result \
	--input_layer=${INPUT_LAYER} \
	--image=${HOME}/dl/label_image_pics/Affenpinscher_00001.jpg \
	--input_width=${INPUT_SIZE} \
	--input_height=${INPUT_SIZE}
\end{lstlisting}

Alternatively with bazel the variables in \listref{label_image} need also to be defined. The call itself is fundamentally the same however the executable binary has to be built before, as shown in \listref{blabel_image}.

\begin{lstlisting}[caption=Build and call of \textit{label\_image}, label=list:blabel_image, language=bash]
	bazel build tensorflow/examples/image_retraining:label_image && \
	bazel-bin/tensorflow/examples/image_retraining/label_image \
		--graph=${HOME}/retrained_graph.pb \
		--labels=${HOME}/retrained_labels.txt \
		--output_layer=final_result \
		--input_layer=${INPUT_LAYER} \
		--image=${HOME}/dl/label_image_pics/Affenpinscher_00001.jpg \
		--input_width=${INPUT_SIZE} \
		--input_height=${INPUT_SIZE}
\end{lstlisting}

If retraining of the model works fine, \textit{label\_image} outputs five labels with the highest and corresponding accuracy which are returned by the model, as shown in following \listref{label_imageOutput}.
\begin{lstlisting}[caption=Output of \textit{label\_image.py}, label=list:label_imageOutput, language=bash]
	Evaluation time (1-image): 0.350s
	
	001 affenpinscher 0.9940035
	038 brussels griffon 0.002003342
	042 cairn terrier 0.0008437461
	100 lowchen 0.00018292216
	099 lhasa apso 0.00012668292
\end{lstlisting}

	\mysubsection{Implementation of an native Android App} Alice
This subchapter describes the processing of the camera input stream and classifying the particular images. Because of the vast extent of the application, the focus lies on the explanation of how the app works and on its important implementation parts. \\

When click on the icon of the app, a camera view showing results in the bottom part is loaded. In the background, the ClassifierActivity is set as the launcher activity in the AndroidManifest.xml which is a file containing all configuration settings for the app. When an activity is loaded for the first time, the onCreate method is invoked. Because the ClassifierActivity extends the CameraActivity the onCreate method of the CameraActivity is executed and loads the layout activity_camera.xml consisting of a container and a result field. Next, the setFragment method of the class CameraActivity is invoked if permission for the camera is given. This causes the replacement of the container with the adapted fragment according to the actual size and orientation of the screen. When the size is set, the method onPreviewSizeChosen is invoked where a TensorFlowImageClassifier is instantiated with following input parameters: MODEL_FILE, LABEL_FILE, INPUT_SIZE, IMAGE_MEAN, IMAGE_STD, INPUT_NAME, OUTPUT_NAME. Details about the Classification follow later in the section.\\

When the fragment was replaced by the CameraActivity, an instance of the class CameraFragment was created. Next, the method onViewCreated of the class CameraFragment is called and a texture view is created. The latter class is part of the Android API and used for frame capturing from an image stream as OPENGL ES texture. Thereby, the most recent image is stored within a texture object. This object is observed by a listener which invokes the method onSurfaceTextureAvailable when the texture object is available. Within this method the camera is opened by the method openCamera(width, height) given the width and height of the camera preview \citep{AndroidDevelopers}. First, the method openCamera sets the camera parameters within setUpCameraOutputs e.g. sensorOrientation, previewSize, cameraId etc. In order to classifiy a given OPENGL ES texture, the coordinate column vectors of the texture must be transformed into the proper sampling location in the streamed texture. So, the matrix has to be prepared with the correct configuration which happens in the method configureTransform. The next important step is done by the camera manager which opens the camera by invoking the method openCamer(cameraId, stateCallback, backgroundHandler). The id represents the specific camera device whereas the stateCallback is necessary to manage the life circle of the camera device and to handle different states of the camera device. The backgroundHandler ensures that the classification is done in background mode. When the camera is opened, the camera preview is started by the method createCameraPreviewSession. Within this method the preview reader is initialized and set to read images from the ImageListener which observes whether an image is available. Therefore, an Imagelistener which was instantiated by the CameraActivity is transfered to the CameraFragment. \\

When an image is available the method onImageAvailable of the class ClassifierActivity is invoked. The image is read by the preview reader and stored in an object. First, the image is preprocessed meaning transfered into planes, stored as bytes, cropped and stored as a Bitmap which is an Android graphic. Google provides an Tensorlflow Mobile API to run tensors on a performance critical device such as mobile devices. To use this API, the depedency has to be added to the build.gradle file \listref{tensorflow_api}. 

\begin{lstlisting}[caption=Tensorflow API in build.gradle, label=list:tensorflow_api, language=java]

	dependencies {
                compile 'org.tensorflow:tensorflow-android:+'
    }
\end{lstlisting}

When including the API, Tensorflow's class Classifier can be used to classify images. Therefore, the TensorFlowImageClassifier which was initialized before invokes the method recognizeImage(croppedBitmap) on the preprocessed image. Within this method, the image data is transfered from int to float. Then, the previous initialized object of the TensorflowInferenceInterface calls the method feed to pass the float values to the input layer of the model. Afterwards, the inferenceInterface invokes the method run with the specified output layer in order to process the classification. Subsequently, the output data is fetched by calling fetch on the output layer \listref{classify_android}.

\begin{lstlisting}[caption=Classifying images by the inferenceInterface, label=list:classify_android, language=java]

    // Copy the input data into TensorFlow.
    Trace.beginSection("feed");
    inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);
    Trace.endSection();

    // Run the inference call.
    Trace.beginSection("run");
    inferenceInterface.run(outputNames, logStats);
    Trace.endSection();

    // Copy the output Tensor back into the output array.
    Trace.beginSection("fetch");
    inferenceInterface.fetch(outputName, outputs);
    Trace.endSection();
\end{lstlisting}

Afterwards, the results are reordered according to the highest probability and returned to the results field for displaying the output.
	\mysubsection{Deployment and Validation} Alice
